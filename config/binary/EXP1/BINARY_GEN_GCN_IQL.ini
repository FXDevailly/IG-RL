[BASE]
n_avg_test : 60
load_model_path : 'config/binary/GCN/Q_L/Train/DIVERSIFIED_NETS/results/BINARY_GEN_GCN_IQL/models_params'
policy : 'binary' # 'free'
save_params_frequency : 100
save_extended_training_stats:True
mode = 'test'
gen_trips_before_exp : True
period = 4
sequential_computation : False
nb_steps_per_exp: 3600
exp_sim_duration = 3600
exp_real_duration = 100000
print_time_gating:False
tests = ['greedy']   #  ['strong_baseline', 'classic']# + [strong_baseline', 'classic',2,3,4,5,6,7,8,9,10]
n_tests = 30
;#, 'strong_baseline', 'classic',2,3,4,5,6,7,8,9,10]
specialist : 1
n_exp = 5
;3600*2
real_net = False
real_net_address : ''
net_name : 'UNIQUE_NET'
exp_name : 'BINARY_GEN_GCN_IQL' 
tb_filename : 'standard'
;#meaningless_test-4-prop-std-resnet-rnn'
tb_embed_viz : False
tb_embed_viz_frequency : 100
save_tb_network_frequency : 100
[INTERFACE]
render : False
save_render : False
save_graph_drawing : False
print_graph_state : False
;PRINTS VECTOR REPRESENTATION OF ALL OBJECTS IN THE GRAPH (EVERY STEP)
print_tl_rewards : False
;# PRINTS REWARDS OF EVERY AGENT (EVERY STEP)
viz_exp_frequency : 150
viz_exp_length : 100
reset_cycle : 7
n_mov_avg : 10
clear : False
clear_after_n_epoch  : 10     
[SIM]
V_ENTER = 35
INNER_LENGTH = 100
LONG_LENGTH = 100
SHORT_LENGTH = 100
N_ROWS = 3
N_COLUMNS = 3
NUM_CARS_LEFT = 0
NUM_CARS_RIGHT = 0
NUM_CARS_TOP = 0
NUM_CARS_BOT = 0
wait_n_steps : 0
sims_per_steps :1
switch_time: 2.0    
;Structure
grid : False
grid_lane_length : 100.0
max_num_lanes : 2 
;# USE FALSE TO USE 1 LANE FOR EVERY EDGE
max_lane_length : 200.0 
min_lane_length : 100.0             
;random net
num_edges_random_net_train:9   
;#150
num_edges_random_net_test:25
;grid net
horizontal_lanes: 1
vertical_lanes: 1
yellow_duration : 5
;-1 #5
min_time_between_actions : 5
;#0 #5
time_between_actions : 1

N_VEH_SAMPLES : 50
PROB_VEH : 0.003
;# 0.003
target_velocity: 50
fringe_factor: 10
demand_duration : 120
demand_variance : 0
lane_demand_variance : 2
min_distance:2
;5#2
Max_Speed: 50
[MODEL]
; GENERAL MODEL 
GCN : True
gaussian_mixture : False 
n_gaussians : 1

[STATE]
;TLs
tl_vars : collections.OrderedDict({'time_since_last_action' :1}) 
;{time_since_last_action :1} #{traci.constants.TL_CURRENT_PHASE : 8 time_since_last_action : 1 x : 1 y : 1}  # name dim_size   /  version continue et dummy / traci.constants.TL_CURRENT_PHASE : 8 traci.constants.TL_CURRENT_PHASE : 1
tl_type: 'controlled'
#Lanes
lane_node_state : True
lane_vars : collections.OrderedDict({'length':1, 'nb_veh' :1, 'avg_speed' : 1}) 
;#nb_veh :1 avg_speed : 1#{nb_veh :1 avg_speed : 1 x :1 y : 1 which_lane : 1 type :4}  # name dim_size
veh_state : False 
;# GIVES A DETAILED VERSION OF the first num_observed veh FOR LANE NODE STATE
num_observed: 2  
;# VEHICLES SEEN PER LANE
lane_per_veh_vars : collections.OrderedDict({'position' :1, 'speed' : 1}) 
;# name dim_size
#Edges
edge_vars : collections.OrderedDict({}) 
;#name dim_size
#Connections 
connection_vars : collections.OrderedDict({'open' :1, 'nb_switch_to_open' : 1, 'current_priority' : 1, 'priority_next_open':1}) 
;#name dim_size
num_observed_next_phases : 12
;# put high number for it not to be constrained 
connection_per_phase_vars : collections.OrderedDict({'type' :4}) 
;# gpriorty r y
phase_state : False
#Phase
phase_vars : collections.OrderedDict({'active' : 1, 'next_yellow':1}) 
;#name dim_size
#Vehicles
ignore_central_vehicles : True
veh_as_nodes : False
veh_vars : collections.OrderedDict({traci.constants.VAR_SPEED : 1, traci.constants.VAR_LANEPOSITION : 1})
;#traci.constants.VAR_SIGNALS : 3# name dim_size
state_embedding_size : 100
state_vars :  ['short_horizontal_idx', 'short_vertical_idx', 'short_current_phases', 'short_cycle_durations']      
;# [current_phases cycle_durations short_current_phases short_cycle_durations]
[ACTION]
discrete: False 
;# ACTION 
control_type : 'choose_next'
;# switch_next 
n_actions :2
n_classes : 1
prediction_size : 1
[REWARD]
reward_type = 'queue_length'
min_dist_delay = 25 # (length of entering area) : In training only, we add the delay of a vehicle to a lane only if it has at least traveled this value (in meters) along the corresponding lane
max_dist_queue = 50 # (length of lane sensors) : In both training and test, vehicles are counted in the queue if the distance which separates them to the intersection (in meters) is equal or inferior to this value 
tau : 0.95
distance_gamma : 0 
time_gamma : 0.95
shaped_reward : True 
std_nb_veh : False
[GRAPH]
generated_graphs : ['tl_graph'] 
graph_of_interest : 'tl_connection_lane_graph' 
;#tl_connection_lane_graph  #  tl_connection_lane_graph
# NODES
node_types_num_bases : -1    
;#node_state_size : 2*8+2        
;# LINKS BETWEEN NODES
norm : False
rel_num_bases : -1
[GCN]
;#tl_connection_lane_graph] # tl_graph lane_graph tl_lane_graph full_graph
resnet : False
state_first_dim_only : True
use_message_module : True
use_attention : False
multidimensional_attention: False 
;# IF MULTIDIM IS FALSE
num_attention_heads : 12
nonlinearity_before_aggregation : True 
bias_before_aggregation : True
normalize attention : True
;# ADDITIVE OR NOT ? 
n_hidden_message : 0
use_aggregation_module : False
;# GRU CELL WITH DIFFERENT PARAMETERS FOR EVERY NODE TYPE 
n_hidden_aggregation : 0
n_hidden_prediction : 0 
;# Q_LEARNER AND ACTOR_CRITIC
n_hidden_value_transition_model : 2
num_propagations : 1 
;# (max(N_ROWS, N_COLUMNS) - 1)  
;# MAX(N_ROWS, N_COLUMNS) - 1
n_convolutional_layers : 2
nn_layers_size :32
n_hidden_layers : 1  
[BASELINE]
#position_threshold = 0.25 # percentage of the lane that has to be 
#stopped_delay_threshold : 0.9
baseline_type : '3sec'
[TRAINING]
#comprehensive_exam_example : True 
;1e12
;#20*60
;# 500 1e12
same_net : True
noisy : True
criterion : 'smooth_l1_loss' #'smooth_l1_loss'
std_lengths: True
std_speeds : True 
separate_memory_buffers : True
prior_exp_replay : False
num_steps_per_batch : 100 
test_frequency : 10 
random_objectives : False
learning_rate : 1e-3
batch_size : 16
;#64 NUMBER OF GRAPHS   # FOR Q-LEARNING
accumulation_steps : 1
dropout : False 
model_train_epochs: 10
stats_per_node : True
clipping : False
clipping_value : 10
l2_norm : 0
[RL]
# RL MODEL CONFIGURATION 
;# Critic
Policy_Type ='Q_Learning'
separate_actor_critic : False
share_initial_params_between_actions : True
Critic_State_Type : 'Shared'  
update_comput_model_frequency : 100
;# IF TRUE RANDOM DIFFERENT OBJECTIVES ARE GIVEN TO EVERY NODE
correct_actions : True 
;# IN Q LEARNING FORCE THE NEXT ACTION TO BE 0 IF NO CHOICE WAS AVAILABLE AT NEXT STATE
train_on_real_choices_only : False 
;# THE LOSS IS ONLY COMPUTED USING NODES WHICH ACTUALLY HAS TO MAKE A CHOICE
target_model_update_frequency: 100
;# 25 50
double_DQN : True
;# Model compute Q  and next argmax Target model computes next Q according to argmax
dueling_DQN : True
;# SEPARATE Q(sa) = V(s) + (A(sa) - mean(A(sai))
value_model_based : False
;# TRAIN EMBEDDING BY ALSO PREDICTING THE NEXT EMBEDDING ACCORDING TO THE CHOSEN ACTION
mini_batch_size : 30
BPTT_steps: 10
ppo_clip_param : 0.2
ppo_epochs : 3
max_buffer_size : 100000
EPS_START : 1
EPS_END : 0.01
EPS_DECAY : 1-1e-4 

